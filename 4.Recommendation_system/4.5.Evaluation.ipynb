{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9891a0bb-ef7f-46a2-b7da-0d238defe9f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4adb702-8468-48af-9fe0-6ea72219789e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Configuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "829175f1-e28f-4d97-90e6-94ae841d073d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nSTAR Recommender System - Evaluation Pipeline\n================================================================================\n\uD83D\uDCCA Interactions table: `bigdata-and-bi`.gold.star_interactions\n\uD83C\uDFAF Recommendations table: `bigdata-and-bi`.gold.star_user_recommendations\n\uD83D\uDCC8 Evaluation metrics table: `bigdata-and-bi`.gold.star_evaluation_metrics\n\uD83D\uDD22 Top-K evaluation: 30\n\uD83D\uDCDD Test strategy: Last interaction per user\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "GOLD_DATABASE = \"`bigdata-and-bi`.gold\"\n",
    "\n",
    "# Input tables\n",
    "INTERACTIONS_TABLE = f\"{GOLD_DATABASE}.star_interactions\"\n",
    "RECOMMENDATIONS_TABLE = f\"{GOLD_DATABASE}.star_user_recommendations\"\n",
    "\n",
    "# Output table for evaluation metrics\n",
    "EVALUATION_METRICS_TABLE = f\"{GOLD_DATABASE}.star_evaluation_metrics\"\n",
    "\n",
    "# Evaluation parameters\n",
    "TOP_K = 30  # Number of recommendations to evaluate\n",
    "MIN_TOTAL_INTERACTIONS = 2  # Users need at least 2 interactions (1 train + 1 test)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STAR Recommender System - Evaluation Pipeline\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Interactions table: {INTERACTIONS_TABLE}\")\n",
    "print(f\"Recommendations table: {RECOMMENDATIONS_TABLE}\")\n",
    "print(f\"Evaluation metrics table: {EVALUATION_METRICS_TABLE}\")\n",
    "print(f\"Top-K evaluation: {TOP_K}\")\n",
    "print(f\"Test strategy: Last interaction per user\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3eaa7b4-114d-4992-a7b2-8bb6414eb8b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3254510d-dda1-4212-aa91-5fba705b8ee8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_evaluation_table_if_not_exists():\n",
    "    \"\"\"\n",
    "    Create evaluation metrics table if it doesn't exist.\n",
    "    Stores historical evaluation results with timestamps.\n",
    "    \"\"\"\n",
    "    schema = StructType([\n",
    "        StructField(\"evaluation_id\", StringType(), False),\n",
    "        StructField(\"evaluation_timestamp\", TimestampType(), False),\n",
    "        StructField(\"test_users_count\", IntegerType(), True),\n",
    "        StructField(\"test_interactions_count\", IntegerType(), True),\n",
    "        StructField(\"users_with_recommendations\", IntegerType(), True),\n",
    "        StructField(\"precision_at_k\", DoubleType(), True),\n",
    "        StructField(\"recall_at_k\", DoubleType(), True),\n",
    "        StructField(\"f1_at_k\", DoubleType(), True),\n",
    "        StructField(\"hit_rate\", DoubleType(), True),\n",
    "        StructField(\"mrr\", DoubleType(), True),  # Mean Reciprocal Rank\n",
    "        StructField(\"map_score\", DoubleType(), True),  # Mean Average Precision\n",
    "        StructField(\"ndcg\", DoubleType(), True),  # Normalized DCG\n",
    "        StructField(\"avg_hit_position\", DoubleType(), True),  # Average rank of hits\n",
    "        StructField(\"coverage\", DoubleType(), True),\n",
    "        StructField(\"avg_recommendations_per_user\", DoubleType(), True),\n",
    "        StructField(\"processing_time_seconds\", DoubleType(), True)\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        spark.table(EVALUATION_METRICS_TABLE)\n",
    "        print(f\"Evaluation metrics table already exists: {EVALUATION_METRICS_TABLE}\")\n",
    "    except Exception:\n",
    "        print(f\"Creating evaluation metrics table: {EVALUATION_METRICS_TABLE}\")\n",
    "        (spark.createDataFrame([], schema)\n",
    "         .write\n",
    "         .format(\"delta\")\n",
    "         .mode(\"overwrite\")\n",
    "         .saveAsTable(EVALUATION_METRICS_TABLE))\n",
    "        print(f\"Created evaluation metrics table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee39f66c-e4c3-45b5-80c6-cc2409e2b110",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1affcf4-b737-4b91-8bd3-9f9f45db1c16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nSTEP 1: LOAD DATA\n================================================================================\n\uD83D\uDCE6 All interactions loaded: 9,489,569\n   Total users: 776,370\n   Total items: 495,063\n\uD83C\uDFAF Recommendations loaded: 23,291,100\n\uD83D\uDC65 Users with recommendations: 776,370\n⏱️  Loading time: 4.15s\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 1: LOAD DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load all interactions\n",
    "all_interactions_df = spark.table(INTERACTIONS_TABLE)\n",
    "total_interactions = all_interactions_df.count()\n",
    "total_users = all_interactions_df.select(\"user_id\").distinct().count()\n",
    "total_items = all_interactions_df.select(\"item_id\").distinct().count()\n",
    "\n",
    "print(f\"All interactions loaded: {total_interactions:,}\")\n",
    "print(f\"   Total users: {total_users:,}\")\n",
    "print(f\"   Total items: {total_items:,}\")\n",
    "\n",
    "# Load recommendations\n",
    "recommendations_df = spark.table(RECOMMENDATIONS_TABLE)\n",
    "total_recommendations = recommendations_df.count()\n",
    "unique_users_with_recs = recommendations_df.select(\"user_id\").distinct().count()\n",
    "\n",
    "print(f\"Recommendations loaded: {total_recommendations:,}\")\n",
    "print(f\"Users with recommendations: {unique_users_with_recs:,}\")\n",
    "\n",
    "print(f\"Loading time: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c033f089-c546-4452-aaca-e9f7cdae7b4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Indentify test interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27ec9c5c-a0c5-46f1-be59-abb62e1665d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nSTEP 2: EXTRACT TEST INTERACTIONS\n================================================================================\n\n\uD83C\uDFAF Test Strategy: Using LAST interaction of each user as test case\n   - Rationale: Last interaction represents user's most recent preference\n   - All previous interactions were used for training\n   - This simulates real-world prediction scenario\n\n✅ Users with >= 2 interactions: 776,370 / 776,370\n\n\uD83D\uDCCA Test Set Statistics:\n   Test interactions: 776,370 (1 per user)\n   Test users       : 776,370\n   Test items       : 256,256\n\n\uD83D\uDCC5 Test Data Temporal Distribution:\n+-------------------+-------------------+\n|earliest           |latest             |\n+-------------------+-------------------+\n|1997-09-26 23:58:29|2023-09-12 00:22:22|\n+-------------------+-------------------+\n\n\n\uD83D\uDCCB Sample test interactions (last interaction per user):\n+----------------------------+-----------------------+----------+-------------------+------+\n|user_id                     |total_user_interactions|item_id   |review_time        |rating|\n+----------------------------+-----------------------+----------+-------------------+------+\n|AHX4SI2LCC3QWH3ET53JGUJNSJSA|12                     |B072BLVM83|2019-03-20 03:04:12|3.0   |\n|AGRIJDNT4HO3QGNBEBJ4HV3BZ5JA|6                      |1616208392|2019-08-26 18:04:19|5.0   |\n|AFHI5NGSCJZD6I3KBPMGWXOGB5IA|5                      |160840840X|2022-10-07 18:14:04|5.0   |\n|AGM2TAZOIMBVJYMTQBBLYT6IUKLQ|17                     |B086WM56PB|2020-10-09 21:22:04|5.0   |\n|AE5Q5FZKAGMWSGWB5WUGIAQQ2PKQ|22                     |1974286851|2018-05-13 14:41:15|3.0   |\n|AEL27UE42FAYJKWVPHQVL6TGTYBQ|49                     |1076038522|2022-02-20 02:17:24|5.0   |\n|AHL5SMNGOMWUJCJOEJOBI4C73PVA|12                     |B01B1OGQH4|2018-03-24 02:55:50|5.0   |\n|AFI5ABOJXFWMYYS6P2VF72RZLPDQ|8                      |1250272726|2022-11-28 13:48:03|5.0   |\n|AECXLP3HTDC5EKKNQB2SGPBPLBAA|78                     |B096R3B19N|2022-05-22 20:50:22|4.0   |\n|AEN3OQS55VZE45OATLO3CS2RSASQ|5                      |B07ZCWWL47|2020-09-14 19:15:45|5.0   |\n+----------------------------+-----------------------+----------+-------------------+------+\nonly showing top 10 rows\n\n\uD83D\uDCD6 Example - Full history for user: AE2224D3S4GTKVFJ5V7ZRQJ7P4FQ\n   Total interactions: 5\n+----------+-------------------+------+\n|item_id   |review_time        |rating|\n+----------+-------------------+------+\n|1476754454|2015-02-05 08:28:27|4.0   |\n|B003KK5RR8|2015-04-11 22:17:56|4.0   |\n|B002U3CBLQ|2015-09-07 21:38:48|5.0   |\n|B01N5XZCTV|2017-08-17 21:32:21|2.0   |\n|038575311X|2019-07-14 00:40:20|4.0   |\n+----------+-------------------+------+\n\n\n   Test interaction (last one):\n+----------+-------------------+------+\n|item_id   |review_time        |rating|\n+----------+-------------------+------+\n|038575311X|2019-07-14 00:40:20|4.0   |\n+----------+-------------------+------+\n\n\n\uD83C\uDFAF Evaluation Mode: OFFLINE (Last Interaction)\n   - Training: All interactions except last per user\n   - Testing: Last interaction per user\n   - Model evaluated on unseen most recent preferences\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: EXTRACT TEST INTERACTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nTest Strategy: Using LAST interaction of each user as test case\")\n",
    "print(f\"   - Rationale: Last interaction represents user's most recent preference\")\n",
    "print(f\"   - All previous interactions were used for training\")\n",
    "print(f\"   - This simulates real-world prediction scenario\")\n",
    "\n",
    "# Count interactions per user\n",
    "user_interaction_counts = all_interactions_df.groupBy(\"user_id\").agg(\n",
    "    F.count(\"*\").alias(\"total_interactions\")\n",
    ")\n",
    "\n",
    "# Filter users with at least MIN_TOTAL_INTERACTIONS\n",
    "users_with_enough_data = user_interaction_counts.filter(\n",
    "    F.col(\"total_interactions\") >= MIN_TOTAL_INTERACTIONS\n",
    ")\n",
    "\n",
    "users_for_eval = users_with_enough_data.count()\n",
    "print(f\"\\nUsers with >= {MIN_TOTAL_INTERACTIONS} interactions: {users_for_eval:,} / {total_users:,}\")\n",
    "\n",
    "# Get the last interaction for each user\n",
    "w_last = Window.partitionBy(\"user_id\").orderBy(F.col(\"unixReviewTime\").desc())\n",
    "\n",
    "test_interactions_df = (all_interactions_df\n",
    "    .join(users_with_enough_data.select(\"user_id\"), \"user_id\", \"inner\")\n",
    "    .withColumn(\"interaction_rank\", F.row_number().over(w_last))\n",
    "    .filter(F.col(\"interaction_rank\") == 1)  # Only keep the last interaction\n",
    "    .select(\"user_id\", \"item_id\", \"unixReviewTime\", \"rating\")\n",
    ")\n",
    "\n",
    "test_count = test_interactions_df.count()\n",
    "test_users = test_interactions_df.select(\"user_id\").distinct().count()\n",
    "test_items = test_interactions_df.select(\"item_id\").distinct().count()\n",
    "\n",
    "print(f\"\\nTest Set Statistics:\")\n",
    "print(f\"   Test interactions: {test_count:,} (1 per user)\")\n",
    "print(f\"   Test users       : {test_users:,}\")\n",
    "print(f\"   Test items       : {test_items:,}\")\n",
    "\n",
    "if test_count == 0:\n",
    "    print(\"No test interactions found. Exiting evaluation.\")\n",
    "    dbutils.notebook.exit(\"No test data to evaluate\")\n",
    "\n",
    "# Show temporal distribution\n",
    "print(f\"\\nTest Data Temporal Distribution:\")\n",
    "test_interactions_df.select(\n",
    "    F.from_unixtime(\"unixReviewTime\").alias(\"review_time\")\n",
    ").agg(\n",
    "    F.min(\"review_time\").alias(\"earliest\"),\n",
    "    F.max(\"review_time\").alias(\"latest\")\n",
    ").show(truncate=False)\n",
    "\n",
    "# Show sample test interactions\n",
    "print(f\"\\nSample test interactions (last interaction per user):\")\n",
    "test_interactions_df.join(\n",
    "    all_interactions_df.groupBy(\"user_id\").agg(\n",
    "        F.count(\"*\").alias(\"total_user_interactions\")\n",
    "    ),\n",
    "    \"user_id\"\n",
    ").select(\n",
    "    \"user_id\",\n",
    "    \"total_user_interactions\",\n",
    "    \"item_id\",\n",
    "    F.from_unixtime(\"unixReviewTime\").alias(\"review_time\"),\n",
    "    \"rating\"\n",
    ").show(10, truncate=False)\n",
    "\n",
    "# Verify: show example of a user's full history vs test\n",
    "sample_user = test_interactions_df.select(\"user_id\").first()[0]\n",
    "print(f\"\\nExample - Full history for user: {sample_user}\")\n",
    "\n",
    "user_history = (all_interactions_df\n",
    "    .filter(F.col(\"user_id\") == sample_user)\n",
    "    .select(\n",
    "        \"item_id\",\n",
    "        F.from_unixtime(\"unixReviewTime\").alias(\"review_time\"),\n",
    "        \"rating\"\n",
    "    )\n",
    "    .orderBy(\"unixReviewTime\")\n",
    ")\n",
    "\n",
    "print(f\"   Total interactions: {user_history.count()}\")\n",
    "user_history.show(truncate=False)\n",
    "\n",
    "user_test = test_interactions_df.filter(F.col(\"user_id\") == sample_user)\n",
    "print(f\"\\n   Test interaction (last one):\")\n",
    "user_test.select(\n",
    "    \"item_id\",\n",
    "    F.from_unixtime(\"unixReviewTime\").alias(\"review_time\"),\n",
    "    \"rating\"\n",
    ").show(truncate=False)\n",
    "\n",
    "print(f\"\\nEvaluation Mode: OFFLINE (Last Interaction)\")\n",
    "print(f\"   - Training: All interactions except last per user\")\n",
    "print(f\"   - Testing: Last interaction per user\")\n",
    "print(f\"   - Model evaluated on unseen most recent preferences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37d7bdee-0641-45ee-955f-b6f9cd20870a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Prepare ground truth and recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daf8804d-b040-4792-8620-e6f151e5fd8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nSTEP 3: PREPARE GROUND TRUTH AND RECOMMENDATIONS\n================================================================================\n\uD83D\uDCDD Ground truth prepared for 776,370 users\n\uD83C\uDFAF Top-30 recommendations prepared for 776,370 users\n✅ Users to evaluate (have both test interaction and recommendations): 776,370\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3: PREPARE GROUND TRUTH AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Ground truth: items that users actually interacted with in test set\n",
    "# Since we only have 1 test interaction per user, this is straightforward\n",
    "ground_truth = (test_interactions_df\n",
    "    .select(\n",
    "        \"user_id\",\n",
    "        F.array(\"item_id\").alias(\"actual_items\")  # Single item wrapped in array\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Ground truth prepared for {ground_truth.count():,} users\")\n",
    "\n",
    "# Get top-K recommendations for test users\n",
    "top_k_recommendations = (recommendations_df\n",
    "    .filter(F.col(\"rank\") <= TOP_K)\n",
    "    .groupBy(\"user_id\")\n",
    "    .agg(F.collect_list(\"item_id\").alias(\"recommended_items\"))\n",
    ")\n",
    "\n",
    "print(f\"Top-{TOP_K} recommendations prepared for {top_k_recommendations.count():,} users\")\n",
    "\n",
    "# Join ground truth with recommendations\n",
    "evaluation_df = ground_truth.join(\n",
    "    top_k_recommendations,\n",
    "    \"user_id\",\n",
    "    \"inner\"  # Only evaluate users who have both test interactions and recommendations\n",
    ")\n",
    "\n",
    "users_to_evaluate = evaluation_df.count()\n",
    "print(f\"Users to evaluate (have both test interaction and recommendations): {users_to_evaluate:,}\")\n",
    "\n",
    "if users_to_evaluate == 0:\n",
    "    print(\"No users to evaluate (no overlap between test users and users with recommendations)\")\n",
    "    dbutils.notebook.exit(\"No users to evaluate\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "975e40b1-88d1-4ae6-bd2d-571a43c7d520",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Calculate Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2296260-a925-442e-af2e-240301c0a82a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nSTEP 4: CALCULATE EVALUATION METRICS\n================================================================================\n\uD83D\uDCCA Evaluating 776,370 users\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 4: CALCULATE EVALUATION METRICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Prepare recommendations with ranks\n",
    "recommendations_with_rank = (recommendations_df\n",
    "    .filter(F.col(\"rank\") <= TOP_K)\n",
    "    .select(\"user_id\", \"item_id\", \"rank\")\n",
    ")\n",
    "\n",
    "# Join ground truth with ranked recommendations\n",
    "evaluation_with_ranks = ground_truth.join(\n",
    "    recommendations_with_rank,\n",
    "    \"user_id\",\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "print(f\"Evaluating {evaluation_with_ranks.select('user_id').distinct().count():,} users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28d202d8-c0cc-4fa0-a421-b55ff46cc042",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Rank-based metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85269eec-0e49-43b0-8cc5-7cb2b54f9ca0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83C\uDFAF Calculating rank-based metrics...\n\n\uD83D\uDCCB Sample user-level metrics:\n+----------------------------+----+---------+------+----+----+------+-------------+\n|user_id                     |hits|precision|recall|mrr |map |ndcg  |hit_positions|\n+----------------------------+----+---------+------+----+----+------+-------------+\n|AF44OEMEO7EDENNDT5Y456V47ONQ|0   |0.0      |0.0   |0.0 |0.0 |0.0   |[]           |\n|AF47PGDGREYUAPBVUC33URD65ZYA|0   |0.0      |0.0   |0.0 |0.0 |0.0   |[]           |\n|AF4ACGTK5AZW4VFSJ6F6MN7QFSOA|0   |0.0      |0.0   |0.0 |0.0 |0.0   |[]           |\n|AF4CU3MFF6IPJLU6RMT3EEIWTPDQ|0   |0.0      |0.0   |0.0 |0.0 |0.0   |[]           |\n|AF4HSDERS7EQJP5FZCOQBIIPWYFA|1   |0.0333   |1.0   |0.25|0.25|0.4307|[4]          |\n+----------------------------+----+---------+------+----+----+------+-------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCalculating rank-based metrics...\")\n",
    "\n",
    "# UDF for calculating comprehensive metrics including rank-based ones\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, ArrayType\n",
    "\n",
    "metrics_schema = StructType([\n",
    "    StructField(\"hits\", IntegerType(), False),\n",
    "    StructField(\"precision\", DoubleType(), False),\n",
    "    StructField(\"recall\", DoubleType(), False),\n",
    "    StructField(\"reciprocal_rank\", DoubleType(), False),  # For MRR\n",
    "    StructField(\"average_precision\", DoubleType(), False),  # For MAP\n",
    "    StructField(\"ndcg\", DoubleType(), False),  # NDCG\n",
    "    StructField(\"hit_positions\", ArrayType(IntegerType()), False)  # Positions of hits\n",
    "])\n",
    "\n",
    "@F.udf(metrics_schema)\n",
    "def calculate_comprehensive_metrics(actual_items, recommended_items_with_ranks):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive metrics including rank-based ones.\n",
    "    \n",
    "    Args:\n",
    "        actual_items: list of actual item IDs\n",
    "        recommended_items_with_ranks: list of tuples [(item_id, rank), ...]\n",
    "    \n",
    "    Returns:\n",
    "        tuple of metrics\n",
    "    \"\"\"\n",
    "    import math\n",
    "    \n",
    "    if not actual_items or not recommended_items_with_ranks:\n",
    "        return (0, 0.0, 0.0, 0.0, 0.0, 0.0, [])\n",
    "    \n",
    "    actual_set = set(actual_items)\n",
    "    \n",
    "    # Sort by rank to ensure correct order\n",
    "    sorted_recs = sorted(recommended_items_with_ranks, key=lambda x: x[1])\n",
    "    \n",
    "    hits = 0\n",
    "    hit_positions = []\n",
    "    first_hit_rank = None\n",
    "    precision_sum = 0.0\n",
    "    dcg = 0.0\n",
    "    \n",
    "    # Calculate metrics in one pass\n",
    "    for idx, (item_id, rank) in enumerate(sorted_recs, 1):\n",
    "        is_hit = item_id in actual_set\n",
    "        \n",
    "        if is_hit:\n",
    "            hits += 1\n",
    "            hit_positions.append(rank)\n",
    "            \n",
    "            # For MRR: record first hit position\n",
    "            if first_hit_rank is None:\n",
    "                first_hit_rank = rank\n",
    "            \n",
    "            # For MAP: accumulate precision at each hit position\n",
    "            precision_at_i = hits / idx\n",
    "            precision_sum += precision_at_i\n",
    "            \n",
    "            # For DCG: rel=1 for hits, rel=0 for misses\n",
    "            dcg += 1.0 / math.log2(rank + 1)\n",
    "    \n",
    "    # Basic metrics\n",
    "    precision = hits / len(sorted_recs) if len(sorted_recs) > 0 else 0.0\n",
    "    recall = hits / len(actual_set) if len(actual_set) > 0 else 0.0\n",
    "    \n",
    "    # MRR (Mean Reciprocal Rank)\n",
    "    reciprocal_rank = 1.0 / first_hit_rank if first_hit_rank else 0.0\n",
    "    \n",
    "    # MAP (Mean Average Precision)\n",
    "    average_precision = precision_sum / len(actual_set) if len(actual_set) > 0 else 0.0\n",
    "    \n",
    "    # NDCG (Normalized Discounted Cumulative Gain)\n",
    "    # IDCG: ideal DCG if all relevant items were at top\n",
    "    ideal_hits = min(len(actual_set), len(sorted_recs))\n",
    "    idcg = sum(1.0 / math.log2(i + 2) for i in range(ideal_hits))\n",
    "    ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "    \n",
    "    return (hits, precision, recall, reciprocal_rank, average_precision, ndcg, hit_positions)\n",
    "\n",
    "# Collect recommendations with ranks per user\n",
    "recommendations_collected = (recommendations_with_rank\n",
    "    .groupBy(\"user_id\")\n",
    "    .agg(\n",
    "        F.collect_list(\n",
    "            F.struct(\"item_id\", \"rank\")\n",
    "        ).alias(\"recommended_items_with_ranks\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Join ground truth with collected recommendations\n",
    "evaluation_df = ground_truth.join(\n",
    "    recommendations_collected,\n",
    "    \"user_id\",\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# Calculate per-user metrics\n",
    "user_metrics_df = evaluation_df.withColumn(\n",
    "    \"metrics\",\n",
    "    calculate_comprehensive_metrics(\n",
    "        F.col(\"actual_items\"), \n",
    "        F.col(\"recommended_items_with_ranks\")\n",
    "    )\n",
    ").select(\n",
    "    \"user_id\",\n",
    "    F.col(\"metrics.hits\").alias(\"hits\"),\n",
    "    F.col(\"metrics.precision\").alias(\"precision\"),\n",
    "    F.col(\"metrics.recall\").alias(\"recall\"),\n",
    "    F.col(\"metrics.reciprocal_rank\").alias(\"reciprocal_rank\"),\n",
    "    F.col(\"metrics.average_precision\").alias(\"average_precision\"),\n",
    "    F.col(\"metrics.ndcg\").alias(\"ndcg\"),\n",
    "    F.col(\"metrics.hit_positions\").alias(\"hit_positions\"),\n",
    "    F.size(\"actual_items\").alias(\"num_actual\")\n",
    ")\n",
    "\n",
    "# Show sample of detailed metrics\n",
    "print(\"\\nSample user-level metrics:\")\n",
    "user_metrics_df.select(\n",
    "    \"user_id\",\n",
    "    \"hits\",\n",
    "    F.round(\"precision\", 4).alias(\"precision\"),\n",
    "    F.round(\"recall\", 4).alias(\"recall\"),\n",
    "    F.round(\"reciprocal_rank\", 4).alias(\"mrr\"),\n",
    "    F.round(\"average_precision\", 4).alias(\"map\"),\n",
    "    F.round(\"ndcg\", 4).alias(\"ndcg\"),\n",
    "    \"hit_positions\"\n",
    ").show(5, truncate=False)\n",
    "\n",
    "# Aggregate metrics across all users\n",
    "aggregated_metrics = user_metrics_df.agg(\n",
    "    F.avg(\"precision\").alias(\"avg_precision\"),\n",
    "    F.avg(\"recall\").alias(\"avg_recall\"),\n",
    "    F.avg(\"reciprocal_rank\").alias(\"avg_mrr\"),\n",
    "    F.avg(\"average_precision\").alias(\"avg_map\"),\n",
    "    F.avg(\"ndcg\").alias(\"avg_ndcg\"),\n",
    "    F.sum(F.when(F.col(\"hits\") > 0, 1).otherwise(0)).alias(\"users_with_hits\"),\n",
    "    F.count(\"*\").alias(\"total_users\")\n",
    ").collect()[0]\n",
    "\n",
    "# Calculate aggregate metrics\n",
    "precision_at_k = aggregated_metrics[\"avg_precision\"]\n",
    "recall_at_k = aggregated_metrics[\"avg_recall\"]\n",
    "mrr = aggregated_metrics[\"avg_mrr\"]\n",
    "map_score = aggregated_metrics[\"avg_map\"]\n",
    "ndcg_score = aggregated_metrics[\"avg_ndcg\"]\n",
    "users_with_hits = aggregated_metrics[\"users_with_hits\"]\n",
    "total_users_evaluated = aggregated_metrics[\"total_users\"]\n",
    "\n",
    "# F1 Score\n",
    "f1_at_k = (2 * precision_at_k * recall_at_k) / (precision_at_k + recall_at_k) \\\n",
    "    if (precision_at_k + recall_at_k) > 0 else 0.0\n",
    "\n",
    "# Hit Rate (percentage of users with at least one hit)\n",
    "hit_rate = users_with_hits / total_users_evaluated if total_users_evaluated > 0 else 0.0\n",
    "\n",
    "# Coverage (percentage of unique items recommended)\n",
    "all_items = all_interactions_df.select(\"item_id\").distinct().count()\n",
    "recommended_items_unique = recommendations_df.select(\"item_id\").distinct().count()\n",
    "coverage = recommended_items_unique / all_items if all_items > 0 else 0.0\n",
    "\n",
    "# Average recommendations per user\n",
    "avg_recs_per_user = recommendations_df.groupBy(\"user_id\").count().agg(\n",
    "    F.avg(\"count\")\n",
    ").collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39fda6a1-21b2-4266-a8eb-0f01f07aa06c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Rank position analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a85d9841-6621-4569-849c-05bbd8852846",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83D\uDCCA Analyzing hit positions...\n\n\uD83D\uDCC8 Hit Distribution by Rank Position:\n+--------+---------+\n|hit_rank|hit_count|\n+--------+---------+\n|       1|    10863|\n|       2|     7034|\n|       3|     4942|\n|       4|     3685|\n|       5|     2662|\n|       6|     2164|\n|       7|     1640|\n|       8|     1424|\n|       9|     1313|\n|      10|     1025|\n|      11|      953|\n|      12|      812|\n|      13|      741|\n|      14|      670|\n|      15|      649|\n|      16|      569|\n|      17|      544|\n|      18|      511|\n|      19|      483|\n|      20|      449|\n+--------+---------+\nonly showing top 20 rows\n\uD83D\uDCCD Average hit position: 6.57\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAnalyzing hit positions...\")\n",
    "\n",
    "# Explode hit positions to analyze distribution\n",
    "hit_position_analysis = (user_metrics_df\n",
    "    .select(\"user_id\", F.explode(\"hit_positions\").alias(\"hit_rank\"))\n",
    "    .groupBy(\"hit_rank\")\n",
    "    .agg(F.count(\"*\").alias(\"hit_count\"))\n",
    "    .orderBy(\"hit_rank\")\n",
    ")\n",
    "\n",
    "print(\"\\nHit Distribution by Rank Position:\")\n",
    "hit_position_analysis.show(20)\n",
    "\n",
    "# Calculate average hit position\n",
    "avg_hit_position = (user_metrics_df\n",
    "    .select(F.explode(\"hit_positions\").alias(\"hit_rank\"))\n",
    "    .agg(F.avg(\"hit_rank\").alias(\"avg_position\"))\n",
    "    .collect()[0][\"avg_position\"]\n",
    ")\n",
    "\n",
    "print(f\"Average hit position: {avg_hit_position:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ccb432e-9518-4b5c-bada-f28e7dbbba98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "049b28ad-cc36-458f-8f30-75d83d4fc589",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nEVALUATION RESULTS\n================================================================================\n\n\uD83D\uDCCA Dataset Statistics:\n  Test users evaluated        : 776,370\n  Test interactions (1 per user): 776,370\n  Users with recommendations  : 776,370\n\n\uD83C\uDFAF Recommendation Quality Metrics:\n  Precision@30             : 0.0020\n  Recall@30                : 0.0599 (Note: 1 test item per user)\n  F1-Score@30              : 0.0039\n  Hit Rate                    : 0.0599 (46,522/776,370)\n\n\uD83C\uDF96️  Rank-Based Metrics (considering position):\n  MRR (Mean Reciprocal Rank)  : 0.0246\n  MAP (Mean Average Precision): 0.0246\n  NDCG@30                  : 0.0324\n  Avg Hit Position            : 6.57\n\n\uD83D\uDCC8 System Metrics:\n  Coverage                    : 0.8882 (439,705/495,063 items)\n  Avg recommendations/user    : 30.00\n\n\uD83D\uDCA1 Interpretation:\n  - Lower Avg Hit Position is better (hits closer to top)\n  - MRR focuses on first hit position\n  - Since we have 1 test item per user, Recall = Hit Rate / K\n  - Hit Rate is the most intuitive metric here\n\n⏱️  Processing time: 159.23s\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Test users evaluated        : {total_users_evaluated:,}\")\n",
    "print(f\"  Test interactions (1 per user): {test_count:,}\")\n",
    "print(f\"  Users with recommendations  : {users_to_evaluate:,}\")\n",
    "\n",
    "print(f\"\\nRecommendation Quality Metrics:\")\n",
    "print(f\"  Precision@{TOP_K}             : {precision_at_k:.4f}\")\n",
    "print(f\"  Recall@{TOP_K}                : {recall_at_k:.4f} (Note: 1 test item per user)\")\n",
    "print(f\"  F1-Score@{TOP_K}              : {f1_at_k:.4f}\")\n",
    "print(f\"  Hit Rate                    : {hit_rate:.4f} ({users_with_hits:,}/{total_users_evaluated:,})\")\n",
    "\n",
    "print(f\"\\nRank-Based Metrics (considering position):\")\n",
    "print(f\"  MRR (Mean Reciprocal Rank)  : {mrr:.4f}\")\n",
    "print(f\"  MAP (Mean Average Precision): {map_score:.4f}\")\n",
    "print(f\"  NDCG@{TOP_K}                  : {ndcg_score:.4f}\")\n",
    "print(f\"  Avg Hit Position            : {avg_hit_position:.2f}\")\n",
    "\n",
    "print(f\"\\nSystem Metrics:\")\n",
    "print(f\"  Coverage                    : {coverage:.4f} ({recommended_items_unique:,}/{all_items:,} items)\")\n",
    "print(f\"  Avg recommendations/user    : {avg_recs_per_user:.2f}\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  - Lower Avg Hit Position is better (hits closer to top)\")\n",
    "print(f\"  - MRR focuses on first hit position\")\n",
    "print(f\"  - Since we have 1 test item per user, Recall = Hit Rate / K\")\n",
    "print(f\"  - Hit Rate is the most intuitive metric here\")\n",
    "\n",
    "processing_time = time.time() - start_time\n",
    "print(f\"\\nProcessing time: {processing_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2368feb0-f268-49b1-b5b2-cef5c0d8b93d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nSTEP 5: SAVE EVALUATION RESULTS\n================================================================================\n✅ Evaluation metrics table already exists: `bigdata-and-bi`.gold.star_evaluation_metrics\n✅ Evaluation results saved to `bigdata-and-bi`.gold.star_evaluation_metrics\n\uD83D\uDCDD Evaluation ID: eval_20251115_095047\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 5: SAVE EVALUATION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "create_evaluation_table_if_not_exists()\n",
    "\n",
    "# Create evaluation record\n",
    "evaluation_id = f\"eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "evaluation_timestamp = datetime.now()\n",
    "\n",
    "evaluation_record = spark.createDataFrame([{\n",
    "    \"evaluation_id\": evaluation_id,\n",
    "    \"evaluation_timestamp\": evaluation_timestamp,\n",
    "    \"test_users_count\": total_users_evaluated,\n",
    "    \"test_interactions_count\": test_count,\n",
    "    \"users_with_recommendations\": users_to_evaluate,\n",
    "    \"precision_at_k\": precision_at_k,\n",
    "    \"recall_at_k\": recall_at_k,\n",
    "    \"f1_at_k\": f1_at_k,\n",
    "    \"hit_rate\": hit_rate,\n",
    "    \"mrr\": mrr,\n",
    "    \"map_score\": map_score,\n",
    "    \"ndcg\": ndcg_score,\n",
    "    \"avg_hit_position\": avg_hit_position,\n",
    "    \"coverage\": coverage,\n",
    "    \"avg_recommendations_per_user\": avg_recs_per_user,\n",
    "    \"processing_time_seconds\": processing_time\n",
    "}])\n",
    "\n",
    "# Append to evaluation metrics table\n",
    "evaluation_record.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(EVALUATION_METRICS_TABLE)\n",
    "\n",
    "print(f\"Evaluation results saved to {EVALUATION_METRICS_TABLE}\")\n",
    "print(f\"Evaluation ID: {evaluation_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52254146-b667-4ff4-9ee4-44df0cfd1b13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nHISTORICAL EVALUATION TRENDS\n================================================================================\n\n\uD83D\uDCCA Last 5 evaluations:\n+--------------------+--------------------------+----------------+---------+------+------+--------+------+------+\n|evaluation_id       |evaluation_timestamp      |test_users_count|precision|recall|f1    |hit_rate|mrr   |ndcg  |\n+--------------------+--------------------------+----------------+---------+------+------+--------+------+------+\n|eval_20251115_095047|2025-11-15 09:50:47.007022|776370          |0.002    |0.0599|0.0039|0.0599  |0.0246|0.0324|\n+--------------------+--------------------------+----------------+---------+------+------+--------+------+------+\n\n\n================================================================================\n✅ EVALUATION COMPLETE\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"HISTORICAL EVALUATION TRENDS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "historical_metrics = spark.table(EVALUATION_METRICS_TABLE) \\\n",
    "    .orderBy(F.col(\"evaluation_timestamp\").desc()) \\\n",
    "    .limit(5)\n",
    "\n",
    "print(\"\\nLast 5 evaluations:\")\n",
    "historical_metrics.select(\n",
    "    \"evaluation_id\",\n",
    "    \"evaluation_timestamp\",\n",
    "    \"test_users_count\",\n",
    "    F.round(\"precision_at_k\", 4).alias(\"precision\"),\n",
    "    F.round(\"recall_at_k\", 4).alias(\"recall\"),\n",
    "    F.round(\"f1_at_k\", 4).alias(\"f1\"),\n",
    "    F.round(\"hit_rate\", 4).alias(\"hit_rate\"),\n",
    "    F.round(\"mrr\", 4).alias(\"mrr\"),\n",
    "    F.round(\"ndcg\", 4).alias(\"ndcg\")\n",
    ").show(truncate=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Evaluation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}