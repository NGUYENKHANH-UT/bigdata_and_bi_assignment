{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd0ce5ad-29a2-4ea6-abed-023e8cf4a824",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cddf51e-05e6-4a1b-a0d9-733ca58f95eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f2ec543-4f18-44c3-bb9d-ba2b80ac848c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import FloatType, StructType, StructField, LongType\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aad8f3cf-9a42-4592-99a1-265dbe3141e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Configuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80b2485c-f7b6-409f-9682-4d94f5f414ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD27 Configuration:\n  Top-K neighbors: 200\n  Vector dimension: 384\n"
     ]
    }
   ],
   "source": [
    "GOLD_DATABASE = \"`bigdata-and-bi`.gold\"\n",
    "\n",
    "# Input tables\n",
    "ITEM_VECTORS_TABLE = f\"{GOLD_DATABASE}.star_item_vectors\"\n",
    "\n",
    "# Output tables\n",
    "SEMANTIC_MATRIX_TABLE = f\"{GOLD_DATABASE}.star_semantic_matrix\"\n",
    "\n",
    "# Top-K parameters\n",
    "TOP_K_NEIGHBORS = 200  # Chỉ lấy 200 neighbors gần nhất cho mỗi item\n",
    "VECTOR_DIMENSION = 384  # Số chiều của vector (cần điều chỉnh theo dữ liệu thực tế)\n",
    "\n",
    "print(\"\uD83D\uDD27 Configuration:\")\n",
    "print(f\"  Top-K neighbors: {TOP_K_NEIGHBORS}\")\n",
    "print(f\"  Vector dimension: {VECTOR_DIMENSION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5778552-2d1a-4e10-b4b4-eaf019c97d0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Helper function defined\n"
     ]
    }
   ],
   "source": [
    "def compute_semantic_matrix_table_exists():\n",
    "    \"\"\"Check if R_S matrix already computed\"\"\"\n",
    "    try:\n",
    "        spark.table(SEMANTIC_MATRIX_TABLE).limit(1).count()\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "print(\"Helper function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40a5683d-4a42-4569-b48e-07628850d491",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Precompute sematic similarity maxtrix R_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d00b2d4f-075a-4d7f-96d4-c0236b7014d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n============================================================\nPrecompute R_S - FAISS Full Dataset with Proper Pagination\n============================================================\n\uD83D\uDD04 Computing R_S matrix for FULL dataset with proper pagination...\n\uD83D\uDCCA Processing FULL dataset: 495,062 items\n\uD83D\uDD27 Processing in 50 batches of 10000 items...\n\uD83D\uDD27 Building FAISS index from batches...\n\uD83D\uDCD0 Vector dimension: 384\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCE6 Batch 1: 10000 items\n\uD83D\uDCCA Batch 1 vectors shape: (10000, 384)\n✅ Added batch 1/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 2: 10000 items\n\uD83D\uDCCA Batch 2 vectors shape: (10000, 384)\n✅ Added batch 2/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 3: 10000 items\n\uD83D\uDCCA Batch 3 vectors shape: (10000, 384)\n✅ Added batch 3/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 4: 10000 items\n\uD83D\uDCCA Batch 4 vectors shape: (10000, 384)\n✅ Added batch 4/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 5: 10000 items\n\uD83D\uDCCA Batch 5 vectors shape: (10000, 384)\n✅ Added batch 5/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 6: 10000 items\n\uD83D\uDCCA Batch 6 vectors shape: (10000, 384)\n✅ Added batch 6/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 7: 10000 items\n\uD83D\uDCCA Batch 7 vectors shape: (10000, 384)\n✅ Added batch 7/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 8: 10000 items\n\uD83D\uDCCA Batch 8 vectors shape: (10000, 384)\n✅ Added batch 8/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 9: 10000 items\n\uD83D\uDCCA Batch 9 vectors shape: (10000, 384)\n✅ Added batch 9/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 10: 10000 items\n\uD83D\uDCCA Batch 10 vectors shape: (10000, 384)\n✅ Added batch 10/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 11: 10000 items\n\uD83D\uDCCA Batch 11 vectors shape: (10000, 384)\n✅ Added batch 11/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 12: 10000 items\n\uD83D\uDCCA Batch 12 vectors shape: (10000, 384)\n✅ Added batch 12/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 13: 10000 items\n\uD83D\uDCCA Batch 13 vectors shape: (10000, 384)\n✅ Added batch 13/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 14: 10000 items\n\uD83D\uDCCA Batch 14 vectors shape: (10000, 384)\n✅ Added batch 14/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 15: 10000 items\n\uD83D\uDCCA Batch 15 vectors shape: (10000, 384)\n✅ Added batch 15/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 16: 10000 items\n\uD83D\uDCCA Batch 16 vectors shape: (10000, 384)\n✅ Added batch 16/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 17: 10000 items\n\uD83D\uDCCA Batch 17 vectors shape: (10000, 384)\n✅ Added batch 17/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 18: 10000 items\n\uD83D\uDCCA Batch 18 vectors shape: (10000, 384)\n✅ Added batch 18/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 19: 10000 items\n\uD83D\uDCCA Batch 19 vectors shape: (10000, 384)\n✅ Added batch 19/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 20: 10000 items\n\uD83D\uDCCA Batch 20 vectors shape: (10000, 384)\n✅ Added batch 20/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 21: 10000 items\n\uD83D\uDCCA Batch 21 vectors shape: (10000, 384)\n✅ Added batch 21/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 22: 10000 items\n\uD83D\uDCCA Batch 22 vectors shape: (10000, 384)\n✅ Added batch 22/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 23: 10000 items\n\uD83D\uDCCA Batch 23 vectors shape: (10000, 384)\n✅ Added batch 23/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 24: 10000 items\n\uD83D\uDCCA Batch 24 vectors shape: (10000, 384)\n✅ Added batch 24/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 25: 10000 items\n\uD83D\uDCCA Batch 25 vectors shape: (10000, 384)\n✅ Added batch 25/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 26: 10000 items\n\uD83D\uDCCA Batch 26 vectors shape: (10000, 384)\n✅ Added batch 26/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 27: 10000 items\n\uD83D\uDCCA Batch 27 vectors shape: (10000, 384)\n✅ Added batch 27/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 28: 10000 items\n\uD83D\uDCCA Batch 28 vectors shape: (10000, 384)\n✅ Added batch 28/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 29: 10000 items\n\uD83D\uDCCA Batch 29 vectors shape: (10000, 384)\n✅ Added batch 29/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 30: 10000 items\n\uD83D\uDCCA Batch 30 vectors shape: (10000, 384)\n✅ Added batch 30/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 31: 10000 items\n\uD83D\uDCCA Batch 31 vectors shape: (10000, 384)\n✅ Added batch 31/50 to index - 10000 vectors\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCE6 Batch 32: 10000 items\n\uD83D\uDCCA Batch 32 vectors shape: (10000, 384)\n✅ Added batch 32/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 33: 10000 items\n\uD83D\uDCCA Batch 33 vectors shape: (10000, 384)\n✅ Added batch 33/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 34: 10000 items\n\uD83D\uDCCA Batch 34 vectors shape: (10000, 384)\n✅ Added batch 34/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 35: 10000 items\n\uD83D\uDCCA Batch 35 vectors shape: (10000, 384)\n✅ Added batch 35/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 36: 10000 items\n\uD83D\uDCCA Batch 36 vectors shape: (10000, 384)\n✅ Added batch 36/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 37: 10000 items\n\uD83D\uDCCA Batch 37 vectors shape: (10000, 384)\n✅ Added batch 37/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 38: 10000 items\n\uD83D\uDCCA Batch 38 vectors shape: (10000, 384)\n✅ Added batch 38/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 39: 10000 items\n\uD83D\uDCCA Batch 39 vectors shape: (10000, 384)\n✅ Added batch 39/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 40: 10000 items\n\uD83D\uDCCA Batch 40 vectors shape: (10000, 384)\n✅ Added batch 40/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 41: 10000 items\n\uD83D\uDCCA Batch 41 vectors shape: (10000, 384)\n✅ Added batch 41/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 42: 10000 items\n\uD83D\uDCCA Batch 42 vectors shape: (10000, 384)\n✅ Added batch 42/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 43: 10000 items\n\uD83D\uDCCA Batch 43 vectors shape: (10000, 384)\n✅ Added batch 43/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 44: 10000 items\n\uD83D\uDCCA Batch 44 vectors shape: (10000, 384)\n✅ Added batch 44/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 45: 10000 items\n\uD83D\uDCCA Batch 45 vectors shape: (10000, 384)\n✅ Added batch 45/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 46: 10000 items\n\uD83D\uDCCA Batch 46 vectors shape: (10000, 384)\n✅ Added batch 46/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 47: 10000 items\n\uD83D\uDCCA Batch 47 vectors shape: (10000, 384)\n✅ Added batch 47/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 48: 10000 items\n\uD83D\uDCCA Batch 48 vectors shape: (10000, 384)\n✅ Added batch 48/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 49: 10000 items\n\uD83D\uDCCA Batch 49 vectors shape: (10000, 384)\n✅ Added batch 49/50 to index - 10000 vectors\n\uD83D\uDCE6 Batch 50: 5062 items\n\uD83D\uDCCA Batch 50 vectors shape: (5062, 384)\n✅ Added batch 50/50 to index - 5062 vectors\n\uD83C\uDFAF FAISS index built with 495062 vectors\n\uD83D\uDCCA Total valid item IDs collected: 495062\n\uD83D\uDD0D Finding similarities batch by batch...\n✅ Processed batch 1/50: 2,000,000 pairs\n✅ Processed batch 2/50: 2,000,000 pairs\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed batch 3/50: 2,000,000 pairs\n✅ Processed batch 4/50: 2,000,000 pairs\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed batch 5/50: 2,000,000 pairs\n✅ Processed batch 6/50: 2,000,000 pairs\n✅ Processed batch 7/50: 2,000,000 pairs\n✅ Processed batch 8/50: 2,000,000 pairs\n✅ Processed batch 9/50: 2,000,000 pairs\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed batch 10/50: 2,000,000 pairs\n✅ Processed batch 11/50: 2,000,000 pairs\n✅ Processed batch 12/50: 2,000,000 pairs\n✅ Processed batch 13/50: 2,000,000 pairs\n✅ Processed batch 14/50: 2,000,000 pairs\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed batch 15/50: 2,000,000 pairs\n✅ Processed batch 16/50: 2,000,000 pairs\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed batch 17/50: 2,000,000 pairs\n✅ Processed batch 18/50: 2,000,000 pairs\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed batch 19/50: 2,000,000 pairs\n✅ Processed batch 20/50: 2,000,000 pairs\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed batch 21/50: 2,000,000 pairs\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed batch 22/50: 2,000,000 pairs\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed batch 23/50: 2,000,000 pairs\n✅ Processed batch 24/50: 2,000,000 pairs\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed batch 25/50: 2,000,000 pairs\n✅ Processed batch 26/50: 2,000,000 pairs\n✅ Processed batch 27/50: 2,000,000 pairs\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed batch 28/50: 2,000,000 pairs\n✅ Processed batch 29/50: 2,000,000 pairs\n✅ Processed batch 30/50: 2,000,000 pairs\n✅ Processed batch 31/50: 2,000,000 pairs\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed batch 32/50: 2,000,000 pairs\n✅ Processed batch 33/50: 2,000,000 pairs\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed batch 34/50: 2,000,000 pairs\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed batch 35/50: 2,000,000 pairs\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2479: UserWarning: Spark Connect Session expired on the server. Please generate a new session by detaching and reattaching the compute if in a Databricks notebook or job or by calling DatabricksSession.builder.getOrCreate() if using Databricks Connect.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mUnknownException\u001B[0m                          Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4857489474097822>, line 105\u001B[0m\n",
       "\u001B[1;32m    100\u001B[0m \u001B[38;5;66;03m# Lấy batch sử dụng row_number()\u001B[39;00m\n",
       "\u001B[1;32m    101\u001B[0m batch_df \u001B[38;5;241m=\u001B[39m item_vectors_numbered\u001B[38;5;241m.\u001B[39mfilter(\n",
       "\u001B[1;32m    102\u001B[0m     (F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrow_num\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m>\u001B[39m start_idx) \u001B[38;5;241m&\u001B[39m (F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrow_num\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m end_idx)\n",
       "\u001B[1;32m    103\u001B[0m )\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mitem_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvector\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m--> 105\u001B[0m batch_data \u001B[38;5;241m=\u001B[39m batch_df\u001B[38;5;241m.\u001B[39mcollect()\n",
       "\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m batch_data:\n",
       "\u001B[1;32m    108\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:1813\u001B[0m, in \u001B[0;36mDataFrame.collect\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   1812\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcollect\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[Row]:\n",
       "\u001B[0;32m-> 1813\u001B[0m     table, schema \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_to_table()\n",
       "\u001B[1;32m   1815\u001B[0m     \u001B[38;5;66;03m# not all datatypes are supported in arrow based collect\u001B[39;00m\n",
       "\u001B[1;32m   1816\u001B[0m     \u001B[38;5;66;03m# here always verify the schema by from_arrow_schema\u001B[39;00m\n",
       "\u001B[1;32m   1817\u001B[0m     schema2 \u001B[38;5;241m=\u001B[39m from_arrow_schema(table\u001B[38;5;241m.\u001B[39mschema, prefer_timestamp_ntz\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:1890\u001B[0m, in \u001B[0;36mDataFrame._to_table\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   1888\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_to_table\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpa.Table\u001B[39m\u001B[38;5;124m\"\u001B[39m, Optional[StructType]]:\n",
       "\u001B[1;32m   1889\u001B[0m     query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n",
       "\u001B[0;32m-> 1890\u001B[0m     table, schema, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execution_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mto_table(\n",
       "\u001B[1;32m   1891\u001B[0m         query, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mobservations\n",
       "\u001B[1;32m   1892\u001B[0m     )\n",
       "\u001B[1;32m   1893\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m table \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   1894\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (table, schema)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1239\u001B[0m, in \u001B[0;36mSparkConnectClient.to_table\u001B[0;34m(self, plan, observations)\u001B[0m\n",
       "\u001B[1;32m   1237\u001B[0m req \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_plan_request_with_metadata()\n",
       "\u001B[1;32m   1238\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mCopyFrom(plan)\n",
       "\u001B[0;32m-> 1239\u001B[0m table, schema, metrics, observed_metrics, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(req, observations)\n",
       "\u001B[1;32m   1241\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n",
       "\u001B[1;32m   1242\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n",
       "\u001B[1;32m   2056\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m   2058\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n",
       "\u001B[1;32m   2060\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n",
       "\u001B[1;32m   2061\u001B[0m     ):\n",
       "\u001B[1;32m   2062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   2063\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
       "\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2434\u001B[0m                 info,\n",
       "\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2438\u001B[0m                 status_code,\n",
       "\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n",
       "\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mUnknownException\u001B[0m: () BAD_REQUEST: session_id is no longer usable. Generate a new session_id by detaching and reattaching the compute and then try again [sessionId=2219a0e5-1cb1-4e8a-ad86-34a4b49e5935, reason=INACTIVITY_TIMEOUT_AND_FAILED_TO_RESTORE]. (requestId=957b4cb3-07f2-416a-9f66-c01f72e3e654)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "UnknownException",
        "evalue": "() BAD_REQUEST: session_id is no longer usable. Generate a new session_id by detaching and reattaching the compute and then try again [sessionId=2219a0e5-1cb1-4e8a-ad86-34a4b49e5935, reason=INACTIVITY_TIMEOUT_AND_FAILED_TO_RESTORE]. (requestId=957b4cb3-07f2-416a-9f66-c01f72e3e654)"
       },
       "metadata": {
        "errorSummary": "() BAD_REQUEST: session_id is no longer usable. Generate a new session_id by detaching and reattaching the compute and then try again [sessionId=2219a0e5-1cb1-4e8a-ad86-34a4b49e5935, reason=INACTIVITY_TIMEOUT_AND_FAILED_TO_RESTORE]. (requestId=957b4cb3-07f2-416a-9f66-c01f72e3e654)"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "INVALID_HANDLE.SESSION_CLOSED",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": "",
        "sqlState": "HY000",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mUnknownException\u001B[0m                          Traceback (most recent call last)",
        "File \u001B[0;32m<command-4857489474097822>, line 105\u001B[0m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;66;03m# Lấy batch sử dụng row_number()\u001B[39;00m\n\u001B[1;32m    101\u001B[0m batch_df \u001B[38;5;241m=\u001B[39m item_vectors_numbered\u001B[38;5;241m.\u001B[39mfilter(\n\u001B[1;32m    102\u001B[0m     (F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrow_num\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m>\u001B[39m start_idx) \u001B[38;5;241m&\u001B[39m (F\u001B[38;5;241m.\u001B[39mcol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrow_num\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m end_idx)\n\u001B[1;32m    103\u001B[0m )\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mitem_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvector\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 105\u001B[0m batch_data \u001B[38;5;241m=\u001B[39m batch_df\u001B[38;5;241m.\u001B[39mcollect()\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m batch_data:\n\u001B[1;32m    108\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:1813\u001B[0m, in \u001B[0;36mDataFrame.collect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1812\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcollect\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[Row]:\n\u001B[0;32m-> 1813\u001B[0m     table, schema \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_to_table()\n\u001B[1;32m   1815\u001B[0m     \u001B[38;5;66;03m# not all datatypes are supported in arrow based collect\u001B[39;00m\n\u001B[1;32m   1816\u001B[0m     \u001B[38;5;66;03m# here always verify the schema by from_arrow_schema\u001B[39;00m\n\u001B[1;32m   1817\u001B[0m     schema2 \u001B[38;5;241m=\u001B[39m from_arrow_schema(table\u001B[38;5;241m.\u001B[39mschema, prefer_timestamp_ntz\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:1890\u001B[0m, in \u001B[0;36mDataFrame._to_table\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1888\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_to_table\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpa.Table\u001B[39m\u001B[38;5;124m\"\u001B[39m, Optional[StructType]]:\n\u001B[1;32m   1889\u001B[0m     query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n\u001B[0;32m-> 1890\u001B[0m     table, schema, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execution_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mto_table(\n\u001B[1;32m   1891\u001B[0m         query, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mobservations\n\u001B[1;32m   1892\u001B[0m     )\n\u001B[1;32m   1893\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m table \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1894\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (table, schema)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1239\u001B[0m, in \u001B[0;36mSparkConnectClient.to_table\u001B[0;34m(self, plan, observations)\u001B[0m\n\u001B[1;32m   1237\u001B[0m req \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_plan_request_with_metadata()\n\u001B[1;32m   1238\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mCopyFrom(plan)\n\u001B[0;32m-> 1239\u001B[0m table, schema, metrics, observed_metrics, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(req, observations)\n\u001B[1;32m   1241\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1242\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   2056\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   2058\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   2060\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   2061\u001B[0m     ):\n\u001B[1;32m   2062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   2063\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2434\u001B[0m                 info,\n\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2438\u001B[0m                 status_code,\n\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mUnknownException\u001B[0m: () BAD_REQUEST: session_id is no longer usable. Generate a new session_id by detaching and reattaching the compute and then try again [sessionId=2219a0e5-1cb1-4e8a-ad86-34a4b49e5935, reason=INACTIVITY_TIMEOUT_AND_FAILED_TO_RESTORE]. (requestId=957b4cb3-07f2-416a-9f66-c01f72e3e654)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Precompute R_S - FAISS Full Dataset with Proper Pagination\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if compute_semantic_matrix_table_exists():\n",
    "    print(f\"R_S matrix already exists: {SEMANTIC_MATRIX_TABLE}\")\n",
    "else:\n",
    "    print(f\"Computing R_S matrix for FULL dataset with proper pagination...\")\n",
    "    \n",
    "    item_vectors_df = spark.table(ITEM_VECTORS_TABLE).select(\"item_id\", \"vector\")\n",
    "    total_count = item_vectors_df.count()\n",
    "    print(f\"Processing FULL dataset: {total_count:,} items\")\n",
    "    \n",
    "    window_spec = Window.orderBy(\"item_id\")\n",
    "    item_vectors_numbered = item_vectors_df.withColumn(\"row_num\", F.row_number().over(window_spec))\n",
    "    \n",
    "    # Memory-optimized batch processing\n",
    "    BATCH_SIZE = 10000\n",
    "    NUM_BATCHES = (total_count + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    \n",
    "    print(f\"Processing in {NUM_BATCHES} batches of {BATCH_SIZE} items...\")\n",
    "    \n",
    "    print(\"Building FAISS index from batches...\")\n",
    "    \n",
    "    sample_row = item_vectors_df.limit(1).collect()[0]\n",
    "    dimension = len(sample_row['vector'].toArray())\n",
    "    print(f\"Vector dimension: {dimension}\")\n",
    "    \n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    all_item_ids = []\n",
    "    \n",
    "    for batch_idx in range(NUM_BATCHES):\n",
    "        start_idx = batch_idx * BATCH_SIZE\n",
    "        end_idx = start_idx + BATCH_SIZE\n",
    "        \n",
    "        batch_df = item_vectors_numbered.filter(\n",
    "            (F.col(\"row_num\") > start_idx) & (F.col(\"row_num\") <= end_idx)\n",
    "        ).select(\"item_id\", \"vector\")\n",
    "        \n",
    "        batch_data = batch_df.collect()\n",
    "        \n",
    "        if not batch_data:\n",
    "            print(f\"Batch {batch_idx + 1} is empty, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Batch {batch_idx + 1}: {len(batch_data)} items\")\n",
    "        \n",
    "        batch_vectors_list = []\n",
    "        batch_item_ids = []\n",
    "        \n",
    "        for row in batch_data:\n",
    "            vector_array = row['vector'].toArray()\n",
    "            if len(vector_array) == dimension:\n",
    "                batch_vectors_list.append(vector_array)\n",
    "                batch_item_ids.append(row['item_id'])\n",
    "        \n",
    "        if not batch_vectors_list:\n",
    "            print(f\"No valid vectors in batch {batch_idx + 1}, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        batch_vectors = np.array(batch_vectors_list, dtype='float32')\n",
    "        print(f\"Batch {batch_idx + 1} vectors shape: {batch_vectors.shape}\")\n",
    "        \n",
    "        faiss.normalize_L2(batch_vectors)\n",
    "        index.add(batch_vectors)\n",
    "        all_item_ids.extend(batch_item_ids)\n",
    "        \n",
    "        print(f\"Added batch {batch_idx + 1}/{NUM_BATCHES} to index - {len(batch_vectors)} vectors\")\n",
    "        \n",
    "        del batch_vectors, batch_data, batch_vectors_list\n",
    "        if batch_idx % 10 == 0: \n",
    "            gc.collect()\n",
    "    \n",
    "    print(f\"\uD83C\uDFAF FAISS index built with {index.ntotal} vectors\")\n",
    "    print(f\"\uD83D\uDCCA Total valid item IDs collected: {len(all_item_ids)}\")\n",
    "    \n",
    "    print(\"\uD83D\uDD0D Finding similarities batch by batch...\")\n",
    "    all_results = []\n",
    "    \n",
    "    for batch_idx in range(NUM_BATCHES):\n",
    "        start_idx = batch_idx * BATCH_SIZE\n",
    "        end_idx = start_idx + BATCH_SIZE\n",
    "        \n",
    "        batch_df = item_vectors_numbered.filter(\n",
    "            (F.col(\"row_num\") > start_idx) & (F.col(\"row_num\") <= end_idx)\n",
    "        ).select(\"item_id\", \"vector\")\n",
    "        \n",
    "        batch_data = batch_df.collect()\n",
    "        \n",
    "        if not batch_data:\n",
    "            continue\n",
    "            \n",
    "        batch_item_ids = []\n",
    "        batch_vectors_list = []\n",
    "        \n",
    "        for row in batch_data:\n",
    "            vector_array = row['vector'].toArray()\n",
    "            if len(vector_array) == dimension:\n",
    "                batch_item_ids.append(row['item_id'])\n",
    "                batch_vectors_list.append(vector_array)\n",
    "        \n",
    "        if not batch_vectors_list:\n",
    "            print(f\"⚠️  No valid vectors in batch {batch_idx + 1} for search, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        batch_vectors = np.array(batch_vectors_list, dtype='float32')\n",
    "        faiss.normalize_L2(batch_vectors)\n",
    "        \n",
    "        k = TOP_K_NEIGHBORS + 1\n",
    "        batch_similarities, batch_indices = index.search(batch_vectors, k)\n",
    "        \n",
    "        batch_results = []\n",
    "        for i in range(len(batch_vectors)):\n",
    "            item_i = batch_item_ids[i]\n",
    "            \n",
    "            for rank in range(k):\n",
    "                j_index = batch_indices[i][rank]\n",
    "                similarity = batch_similarities[i][rank]\n",
    "                \n",
    "                if 0 <= j_index < len(all_item_ids) and similarity > 0.1:\n",
    "                    item_j = all_item_ids[j_index]\n",
    "                    if item_i != item_j:\n",
    "                        batch_results.append((item_i, item_j, float(similarity)))\n",
    "        \n",
    "        all_results.extend(batch_results)\n",
    "        \n",
    "        del batch_vectors, batch_similarities, batch_indices, batch_data, batch_vectors_list\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"✅ Processed batch {batch_idx + 1}/{NUM_BATCHES}: {len(batch_results):,} pairs\")\n",
    "    \n",
    "    print(f\"\uD83D\uDCC8 Generated {len(all_results):,} similarity pairs total\")\n",
    "    \n",
    "    print(\"\uD83D\uDD04 Creating Spark DataFrame...\")\n",
    "    \n",
    "    from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"item_i\", StringType(), True),\n",
    "        StructField(\"item_j\", StringType(), True),\n",
    "        StructField(\"semantic_score\", FloatType(), True)\n",
    "    ])\n",
    "    \n",
    "    CHUNK_SIZE = 2000000\n",
    "    num_chunks = (len(all_results) + CHUNK_SIZE - 1) // CHUNK_SIZE\n",
    "    \n",
    "    final_df = None\n",
    "    for chunk_idx in range(num_chunks):\n",
    "        start_idx = chunk_idx * CHUNK_SIZE\n",
    "        end_idx = min((chunk_idx + 1) * CHUNK_SIZE, len(all_results))\n",
    "        chunk_results = all_results[start_idx:end_idx]\n",
    "        \n",
    "        chunk_df = spark.createDataFrame(chunk_results, schema)\n",
    "        \n",
    "        if final_df is None:\n",
    "            final_df = chunk_df\n",
    "        else:\n",
    "            final_df = final_df.union(chunk_df)\n",
    "        \n",
    "        print(f\"Created DataFrame chunk {chunk_idx + 1}/{num_chunks}\")\n",
    "        del chunk_df, chunk_results\n",
    "        gc.collect()\n",
    "    \n",
    "    # top-k neighbors\n",
    "    print(\"\uD83C\uDFC6 Selecting top-K neighbors...\")\n",
    "    window_spec = Window.partitionBy(\"item_i\").orderBy(F.desc(\"semantic_score\"))\n",
    "    semantic_matrix_topk = final_df.withColumn(\n",
    "        \"rank\", F.row_number().over(window_spec)\n",
    "    ).filter(F.col(\"rank\") <= TOP_K_NEIGHBORS).drop(\"rank\")\n",
    "    \n",
    "    final_count = semantic_matrix_topk.count()\n",
    "    print(f\"Final R_S matrix contains {final_count:,} pairs\")\n",
    "    \n",
    "    distinct_items = semantic_matrix_topk.select(\"item_i\").distinct().count()\n",
    "    coverage = distinct_items / total_count * 100\n",
    "    print(f\"Coverage: {distinct_items:,}/{total_count:,} items ({coverage:.2f}%)\")\n",
    "    \n",
    "    # Save\n",
    "    print(\"Saving final R_S matrix...\")\n",
    "    semantic_matrix_topk.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(SEMANTIC_MATRIX_TABLE)\n",
    "    \n",
    "    # Final cleanup\n",
    "    del all_results, final_df, semantic_matrix_topk, index, all_item_ids, item_vectors_numbered\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"Saved FULL R_S matrix to {SEMANTIC_MATRIX_TABLE}\")\n",
    "    print(f\"Success! Processed {total_count:,} items with {final_count:,} similarity pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "552a4b44-604f-4488-9050-e935378c9c9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Verify\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Verification Results\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "semantic_matrix = spark.table(SEMANTIC_MATRIX_TABLE)\n",
    "\n",
    "total_pairs = semantic_matrix.count()\n",
    "distinct_items_i = semantic_matrix.select(\"item_i\").distinct().count()\n",
    "item_count = spark.table(ITEM_VECTORS_TABLE).count()\n",
    "\n",
    "print(f\"\uD83D\uDCC8 Total pairs in R_S: {total_pairs:,}\")\n",
    "print(f\"\uD83D\uDCCA Distinct items in item_i: {distinct_items_i:,}\")\n",
    "print(f\"\uD83D\uDCCA Total items: {item_count:,}\")\n",
    "print(f\"\uD83D\uDCCA Coverage: {distinct_items_i/item_count*100:.2f}%\")\n",
    "\n",
    "# similarity scores\n",
    "print(\"\\nSemantic score distribution:\")\n",
    "stats = semantic_matrix.select(\n",
    "    F.min(\"semantic_score\").alias(\"min_score\"),\n",
    "    F.max(\"semantic_score\").alias(\"max_score\"),\n",
    "    F.mean(\"semantic_score\").alias(\"mean_score\"),\n",
    "    F.stddev(\"semantic_score\").alias(\"std_score\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"  Min score: {stats['min_score']:.4f}\")\n",
    "print(f\"  Max score: {stats['max_score']:.4f}\")\n",
    "print(f\"  Mean score: {stats['mean_score']:.4f}\")\n",
    "print(f\"  Std score: {stats['std_score']:.4f}\")\n",
    "\n",
    "print(\"\\n\uD83D\uDD0D Sample of R_S matrix:\")\n",
    "semantic_matrix.orderBy(F.rand()).limit(10).show()\n",
    "\n",
    "avg_neighbors = semantic_matrix.groupBy(\"item_i\").count().select(F.avg(\"count\")).collect()[0][0]\n",
    "print(f\"\uD83D\uDCCA Average neighbors per item: {avg_neighbors:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "R_S_score",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}