{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28556d88-a322-4939-ab19-57551f44c451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ETL RS silver to gold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5804af7-974e-4cd0-a302-d96627b171d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab376e68-0f7e-453e-ba79-6a96f880478b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType, StructType, StructField, MapType, DoubleType\n",
    "from delta.tables import DeltaTable\n",
    "import ast\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f29266b-6711-4c57-91fa-43f31959f016",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "561ae0a0-8550-4a1d-b4d7-603c7d8dc896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define database and table names\n",
    "SILVER_DATABASE = \"`bigdata-and-bi`.silver\"\n",
    "GOLD_DATABASE = \"`bigdata-and-bi`.gold\"\n",
    "\n",
    "REVIEWS_TABLE = f\"{SILVER_DATABASE}.reviews_clean\"\n",
    "ITEMS_TABLE = f\"{SILVER_DATABASE}.items_clean\"\n",
    "\n",
    "GOLD_INTERACTIONS_TABLE = f\"{GOLD_DATABASE}.star_interactions\"\n",
    "GOLD_ITEMS_TABLE = f\"{GOLD_DATABASE}.star_items\"\n",
    "\n",
    "# Checkpoint locations are required for streaming queries\n",
    "INTERACTIONS_CHECKPOINT = \"/Volumes/bigdata-and-bi/gold/star_checkpoints/interactions_silver_to_gold\"\n",
    "ITEMS_CHECKPOINT = \"/Volumes/bigdata-and-bi/gold/star_checkpoints/items_silver_to_gold\"\n",
    "\n",
    "K_CORE_VALUE = 3\n",
    "\n",
    "# Ensure the gold database exists\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {GOLD_DATABASE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "263bab8a-dfa6-4635-b6ea-67b20abe2d9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### UDFs for Item Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6870b9a6-f1d7-40c7-bdff-c6b2d8dce716",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# These UDFs parse complex string-formatted columns from the source data\n",
    "\n",
    "@F.udf(StringType())\n",
    "def parse_features_udf(features_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts 'features' string-list into a single text sentence.\n",
    "    E.g., \"['['feat1', 'feat2']']\" -> \"feat1. feat2.\"\n",
    "    \"\"\"\n",
    "    if not features_str or features_str == \"[]\":\n",
    "        return \"\"\n",
    "    try:\n",
    "        # First eval: \"['[...]']\" -> ['[...]']\n",
    "        outer_list = ast.literal_eval(features_str)\n",
    "        if not outer_list:\n",
    "            return \"\"\n",
    "        \n",
    "        # Second eval: \"[...]\" -> [...]\n",
    "        inner_list_str = outer_list[0]\n",
    "        inner_list = ast.literal_eval(inner_list_str)\n",
    "        \n",
    "        # Join into sentences\n",
    "        return \". \".join(inner_list)\n",
    "    except Exception:\n",
    "        return \"\" # Return empty string on parsing error\n",
    "\n",
    "@F.udf(StringType())\n",
    "def parse_details_udf(details_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts key fields from the 'details' dict-string\n",
    "    and formats them as text.\n",
    "    \"\"\"\n",
    "    if not details_str:\n",
    "        return \"\"\n",
    "    try:\n",
    "        # 'details' is a string representation of a dict\n",
    "        details_map = ast.literal_eval(details_str)\n",
    "        if not isinstance(details_map, dict):\n",
    "            return \"\"\n",
    "            \n",
    "        selected = []\n",
    "        # Select key details as requested\n",
    "        if \"Language\" in details_map:\n",
    "            selected.append(f\"Language: {details_map['Language']}\")\n",
    "        if \"Dimensions\" in details_map:\n",
    "            selected.append(f\"Dimensions: {details_map['Dimensions']}\")\n",
    "        if \"Paperback\" in details_map:\n",
    "            selected.append(f\"Pages: {details_map['Paperback']}\")\n",
    "            \n",
    "        return \". \".join(selected)\n",
    "    except Exception:\n",
    "        return \"\" # Return empty string on parsing error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5210004c-56af-43d9-915b-8524631102c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Stream 1: Process and Merge Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30c66500-30b2-4313-8c23-0e3923316a22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Items stream: `bigdata-and-bi`.silver.items_clean -> `bigdata-and-bi`.gold.star_items\nItems streaming query started (will stop when micro-batch is complete).\n"
     ]
    }
   ],
   "source": [
    "print(f\"Setting up Items stream: {ITEMS_TABLE} -> {GOLD_ITEMS_TABLE}\")\n",
    "\n",
    "def process_items_batch(micro_batch_df, batch_id):\n",
    "    \"\"\"\n",
    "    This function processes a micro-batch of item data.\n",
    "    It aggregates metadata by 'parent_asin' and merges into the Gold table.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Aggregation Step ---\n",
    "    items_aggregated_df = micro_batch_df.groupBy(\"parent_asin\").agg(\n",
    "        F.first(\"title\", ignorenulls=True).alias(\"title\"),\n",
    "        F.first(\"description\", ignorenulls=True).alias(\"description\"),\n",
    "        F.first(\"features\", ignorenulls=True).alias(\"features\"),\n",
    "        F.first(\"categories\", ignorenulls=True).alias(\"categories\"),\n",
    "        F.first(\"store\", ignorenulls=True).alias(\"store\"),\n",
    "        F.first(\"details\", ignorenulls=True).alias(\"details\"),\n",
    "        F.avg(\"average_rating\").alias(\"average_rating\"),\n",
    "        F.avg(\"price\").alias(\"price\")\n",
    "    )\n",
    "    \n",
    "    # --- Apply transformations on aggregated data ---\n",
    "    # We coalesce text fields to empty strings,\n",
    "    # but keep numerical fields as NULL if they are missing\n",
    "    items_processed_df = items_aggregated_df.select(\n",
    "        F.col(\"parent_asin\").alias(\"item_id\"), \n",
    "        F.coalesce(F.col(\"title\"), F.lit(\"\")).alias(\"title\"),\n",
    "        F.coalesce(F.array_join(F.col(\"description\"), \" \"), F.lit(\"\")).alias(\"description\"),\n",
    "        parse_features_udf(F.col(\"features\")).alias(\"features\"),\n",
    "        F.coalesce(F.array_join(F.col(\"categories\"), \" > \"), F.lit(\"\")).alias(\"categories\"),\n",
    "        F.coalesce(F.col(\"store\"), F.lit(\"\")).alias(\"store\"),\n",
    "        parse_details_udf(F.col(\"details\")).alias(\"details\"),\n",
    "        F.col(\"average_rating\").cast(DoubleType()).alias(\"average_rating\"),\n",
    "        F.col(\"price\").cast(DoubleType()).alias(\"price\")\n",
    "    )\n",
    "    \n",
    "    # F.concat_ws(\"\\n\", ...) joins non-null lines with a newline\n",
    "    # and automatically skips any line that evaluates to NULL.\n",
    "    final_items_df = items_processed_df.withColumn(\"prompt_text\",\n",
    "        F.concat_ws(\"\\n\",\n",
    "            # Only add the line if the text value is not empty\n",
    "            F.when(F.col(\"title\") != \"\", \n",
    "                   F.concat(F.lit(\"title: \"), F.col(\"title\"))),\n",
    "                   \n",
    "            F.when(F.col(\"description\") != \"\", \n",
    "                   F.concat(F.lit(\"description: \"), F.col(\"description\"))),\n",
    "                   \n",
    "            F.when(F.col(\"features\") != \"\", \n",
    "                   F.concat(F.lit(\"features: \"), F.col(\"features\"))),\n",
    "                   \n",
    "            F.when(F.col(\"categories\") != \"\", \n",
    "                   F.concat(F.lit(\"categories: \"), F.col(\"categories\"))),\n",
    "                   \n",
    "            F.when(F.col(\"store\") != \"\", \n",
    "                   F.concat(F.lit(\"store: \"), F.col(\"store\"))),\n",
    "                   \n",
    "            F.when(F.col(\"details\") != \"\", \n",
    "                   F.concat(F.lit(\"details: \"), F.col(\"details\"))),\n",
    "                   \n",
    "            # Only add the line if the numeric value is not NULL\n",
    "            F.when(F.col(\"average_rating\").isNotNull(), \n",
    "                   F.concat(F.lit(\"average_rating: \"), F.col(\"average_rating\").cast(\"string\"))),\n",
    "                   \n",
    "            F.when(F.col(\"price\").isNotNull(), \n",
    "                   F.concat(F.lit(\"price: \"), F.col(\"price\").cast(\"string\")))\n",
    "        )\n",
    "    ).select(\n",
    "        \"item_id\", \"title\", \"description\", \"features\", \"categories\", \n",
    "        \"store\", \"details\", \"average_rating\", \"price\",\n",
    "        \"prompt_text\"\n",
    "    )\n",
    "    \n",
    "    # --- Merge (Upsert) into the Gold table ---\n",
    "    # This handles both new items and updates to existing items\n",
    "    try:\n",
    "        gold_table = DeltaTable.forName(spark, GOLD_ITEMS_TABLE)\n",
    "        \n",
    "        (gold_table.alias(\"gold\")\n",
    "         .merge(final_items_df.alias(\"silver\"), \"gold.item_id = silver.item_id\")\n",
    "         .whenMatchedUpdateAll()  # Update if item metadata changed\n",
    "         .whenNotMatchedInsertAll() # Insert if new item\n",
    "         .execute()\n",
    "        )\n",
    "        print(f\"Items Batch {batch_id}: Merged {final_items_df.count()} records into {GOLD_ITEMS_TABLE}.\")\n",
    "    except Exception as e:\n",
    "        error_str = str(e)\n",
    "        if \"DELTA_MISSING_DELTA_TABLE\" in error_str or \"DELTA_TABLE_NOT_FOUND\" in error_str:\n",
    "            print(f\"Items Batch {batch_id}: Gold table not found, creating {GOLD_ITEMS_TABLE}...\")\n",
    "            (final_items_df.write.format(\"delta\")\n",
    "             .mode(\"overwrite\")\n",
    "             .saveAsTable(GOLD_ITEMS_TABLE))\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "# Read from Silver items table as a stream\n",
    "items_stream_df = spark.readStream.table(ITEMS_TABLE)\n",
    "\n",
    "# Write the stream using foreachBatch and trigger(availableNow=True)\n",
    "# This makes it a perfect scheduled job\n",
    "items_stream_query = (items_stream_df.writeStream\n",
    "                      .foreachBatch(process_items_batch)\n",
    "                      .option(\"checkpointLocation\", ITEMS_CHECKPOINT)\n",
    "                      .trigger(availableNow=True)\n",
    "                      .start()\n",
    "                     )\n",
    "\n",
    "print(\"Items streaming query started (will stop when micro-batch is complete).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f58f2c6-e46d-4b58-a2c2-ef03eaad10cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Stream 2: Process and Merge Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc974e10-bb0b-49ed-8418-c2255a4a80f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Interactions stream: `bigdata-and-bi`.silver.reviews_clean -> `bigdata-and-bi`.gold.star_interactions\nInteractions streaming query started (will stop when micro-batch is complete).\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/11 10:06:40 Spark Server has not sent updates for Streaming Query 0b302b47-669d-404d-97ba-1dbf08bc3aab in 60 seconds, but the query is still active. Marking query as in-progress. Spark Session ID is b26caff5-8464-44e4-85f6-7bf78ba48b6f. This is typically not a problem.\n25/11/11 10:07:05 Spark Server has not sent updates for Streaming Query 0b302b47-669d-404d-97ba-1dbf08bc3aab in 60 seconds, but the query is still active. Marking query as in-progress. Spark Session ID is b26caff5-8464-44e4-85f6-7bf78ba48b6f. This is typically not a problem.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming ETL job complete for this run.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Setting up Interactions stream: {REVIEWS_TABLE} -> {GOLD_INTERACTIONS_TABLE}\")\n",
    "\n",
    "def process_interactions_batch(micro_batch_df, batch_id):\n",
    "    \"\"\"\n",
    "    This function processes a micro-batch of interaction data\n",
    "    and merges it into the Gold table.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select and transform columns\n",
    "    interactions_df = micro_batch_df.select(\n",
    "        F.col(\"parent_asin\").alias(\"item_id\"),\n",
    "        F.col(\"user_id\"),\n",
    "        F.to_timestamp(F.col(\"reviewTimestamp\")).cast(\"long\").alias(\"unixReviewTime\"),\n",
    "        F.col(\"rating\").cast(DoubleType()) # Keep original rating\n",
    "    )\n",
    "    \n",
    "    # --- Merge (Upsert) into the Gold table ---\n",
    "    # We match on a composite key (user, item, time) to ensure\n",
    "    # idempotency and prevent duplicate interactions.\n",
    "    try:\n",
    "        gold_table = DeltaTable.forName(spark, GOLD_INTERACTIONS_TABLE)\n",
    "        \n",
    "        (gold_table.alias(\"gold\")\n",
    "         .merge(interactions_df.alias(\"silver\"), \n",
    "                \"gold.user_id = silver.user_id AND \" +\n",
    "                \"gold.item_id = silver.item_id AND \" +\n",
    "                \"gold.unixReviewTime = silver.unixReviewTime\")\n",
    "         .whenMatchedUpdateAll() # e.g., if rating was corrected\n",
    "         .whenNotMatchedInsertAll() # New interaction\n",
    "         .execute()\n",
    "        )\n",
    "        print(f\"Interactions Batch {batch_id}: Merged {interactions_df.count()} records into {GOLD_INTERACTIONS_TABLE}.\")\n",
    "    except Exception as e:\n",
    "        error_str = str(e)\n",
    "        if \"DELTA_MISSING_DELTA_TABLE\" in error_str or \"DELTA_TABLE_NOT_FOUND\" in error_str:\n",
    "            print(f\"Interactions Batch {batch_id}: Gold table not found, creating {GOLD_INTERACTIONS_TABLE}...\")\n",
    "            (interactions_df.write.format(\"delta\")\n",
    "             .mode(\"overwrite\")\n",
    "             .saveAsTable(GOLD_INTERACTIONS_TABLE))\n",
    "        else:\n",
    "            raise e\n",
    "# Read from Silver reviews table as a stream\n",
    "reviews_stream_df = spark.readStream.table(REVIEWS_TABLE)\n",
    "\n",
    "# Write the stream using foreachBatch\n",
    "reviews_stream_query = (reviews_stream_df.writeStream\n",
    "                        .foreachBatch(process_interactions_batch)\n",
    "                        .option(\"checkpointLocation\", INTERACTIONS_CHECKPOINT)\n",
    "                        .trigger(availableNow=True)\n",
    "                        .start()\n",
    "                       )\n",
    "                       \n",
    "print(\"Interactions streaming query started (will stop when micro-batch is complete).\")\n",
    "\n",
    "# Wait for the streams to finish processing the current batch\n",
    "items_stream_query.awaitTermination()\n",
    "reviews_stream_query.awaitTermination()\n",
    "\n",
    "print(\"Streaming ETL job complete for this run.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ETL_RS_silver_to_gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}